\section{Marco Aplicativo}

\subsection{Método de Desarrollo}

Para llevar a cabo el cumplimiento de los objetivos planteados en el Capítulo 2, es necesario definir un
esquema o metodología de trabajo que permita la elaboración de un conjunto de
lineamientos de manera estructurada y organizada. Por lo tanto, se hace uso de el método de
Desarrollo Rápido de Aplicaciones (RAD), el cual permite la iteración rápida y continua
de pequeños objetivos para alcanzar la meta final.

RAD es un proceso de desarrollo de software que
integra un conjunto de técnicas, lineamientos y herramientas que permiten llevar a cabo, en
cortos periodos de tiempo, la implementación de funcionalidades en un sistema, de tal manera que
satisfacen las necesidades del cliente \cite{23}.
Siguiendo esta metodología, el software evoluciona y crece durante el proceso de desarrollo en base a la retro alimentación que se tiene con el cliente.
De esta manera, se realizan múltiples entregas de una tarea que
contiene las nuevas funcionalidades esperadas.
Cada una de estas tareas cuenta con instrumento de documentación al cuál llamaremos historias.
Estas historias describen los detalles técnicos e categorizan las tareas en subgrupos.

Por medio de RAD, fue posible la implementación de la solución al problema previamente planteado, lo cuál fue desarrollada como un servicio web API conformado por:

\begin{itemize}

\item Un modulo de extracción de revisiones de artículos wiki.

\item Un modulo de almacenamiento de datos distribuido por medio de MongoDB.

\item Un modulo de consulta de los artículos wiki extraídos y sus correspondientes revisiones.

\item Y, por último, un modulo de consulta de métricas generales de las revisiones extraídas, incluyendo: totalizaciones, promedios y moda.

\end{itemize}

La siguiente figura representa la vista general de la arquitectura de la aplicación, y como la misma interactuá con
las aplicaciones clientes que consumen el servicio:

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/diagram_general}
	\caption{Arquitectura general de la aplicación}
	\label{fig:diagram_general}

\end{figure}

\subsection{Servidor Web}

El servidor web que atiende las solicitudes en primera instancia es Nginx,
el cual funciona como un intermediario entre el cliente y el API que responde dicha solicitud.
Nginx permite balancear la carga de trabajo entre múltiples servidores que contienen la aplicación mediante el algoritmo de round robin,
de tal forma que cada servidor es utilizado de forma equitativa, tal como se muestra en la siguiente figura:

\begin{figure}[H]
	\centering
		\includegraphics[width=0.9\textwidth]{figures/round_robin}
	\caption{Algoritmo de round robin para la distribución de peticiones de Nginx}
	\label{fig:round_robin}
\end{figure}

Una vez la petición ha sido reasignada, es atendida por medio del servidor web UWSGI, y en conjunto
con Flask y el resto de las tecnologías incluidas en el API, genera una respuesta de vuelta al cliente.

\subsection{API}

El API diseñado para realizar las labores de extracción y consulta de las revisiones, hace uso del lenguaje de programación Python \texttt{2.7.10} y el uso del framework Flask \texttt{v0.12}.
Ambas tecnologías permiten la asignación de rutas específicas para cada módulo
y la atención de peticiones.

La interacción entre el servicio y una aplicación cliente puede ser apreciada en la figura \ref{fig:diagram_api_1}.

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/diagram_api_1}
	\caption{Interacción entre una aplicación cliente y el API}
	\label{fig:diagram_api_1}
\end{figure}

Esta interacción se basa en la petición de un recurso o de un proceso, por parte de una aplicación cliente, a una ruta del servidor web (endpoint).
Cada petición puede incluir parámetros opcionales para la paginación, filtración de resultados, como por ejemplo, el idioma del artículo a extraer.

Las rutas de acceso que ofrece el API son las siguientes:

\begin{itemize}
	\item \texttt{/api/v1/articles}. Listado de todos los artículos extraídos.
	\item \texttt{/api/v1/revisions}. Listado de las revisiones de los artículos extraídos.
	\item \texttt{/api/v1/extract}. Extracción de revisiones de artículos wiki.
	\item \texttt{/api/v1/avg}. Cálculo del promedio de revisiones de un artículo por rango fechas.
	\item \texttt{/api/v1/count}. Cálculo de número de revisiones de un artículo por fecha.
	\item \texttt{/api/v1/mode}. Cálculo de las revisiones mas extraídas por rango de fecha.
	\item \texttt{/api/v1/status}. Indica el estado del proceso de extracción.
	\item \texttt{/api/v1/query}. Permite la ejecución de consultas personalizadas a la base de datos.
\end{itemize}

\subsubsection{Extracción de Historiales}

Para la extracción de historiales se hace uso de la ruta de acceso \texttt{/api/v1/extract}, la cual
tiene como parámetros:
\texttt{title}, que se refiere al título del del artículo; \texttt{url}, representa la ruta completa de un artículo, por ejemplo:
\texttt{https://en.wikipedia.org/wiki/The\_Lord\_of\_the\_Rings}; y por último, \texttt{locale}, el cual índica el idioma del artículo y
es completamente opcional, puesto que el sistema asume el inglés como lenguaje por defecto o lo extrae del parámetro
\texttt{url}.
Es necesario proporcionar el parámetro \texttt{url} o \texttt{title} de forma obligatoria para poder llevar a cabo la extracción.

En la siguiente figura se puede apreciar dos ejemplos del uso de los parámetros \texttt{url}, \texttt{title} y \texttt{locale}:

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/extract_url_format}
	\caption{Parámetros para la extracción de historiales}
	\label{fig:extract_url_format}
\end{figure}

La ejecución de esta tarea se lleva a cabo por medio de Celery y RabbitMQ.
Con estas tecnologías, múltiples computadores ejecutan un proceso de Celery, los
cuales están conectados entre si gracias a RabbitMQ, escuchando constantemente los mensajes entrantes de este servicio.
Una vez recibida la petición por el API, se genera una tarea de extracción por medio de Celery, la cual tiene un identificador único. Esta tarea es encolada en una lista de
espera que es atendida por uno de los trabajadores conectados a RabbitMQ y es ejecutada en segundo plano.
Posteriormente, el API crea una respuesta a la petición generando una ruta para consultar el progreso
del proceso de extracción, la cual luce de la siguiente manera:

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/extract_response}
	\caption{Respuesta a una petición de extracción de historiales de un artículo}
	\label{fig:extract_response}
\end{figure}

Durante el proceso de extracción, se realizan peticiones hacia el API de Wikipedia.
Se toma como URL por defecto la dirección https://en.wikipedia.org/w/api.php, en conjunto con diversos parametros, tales como:

\begin{itemize}
	\item \textbf{action}: tipo de petición que se realiza sobre este API, en este caso se hace uso del valor \texttt{query} para indicar que solo se realizará una consulta.

	\item \textbf{format}: formato de la respuesta obtenida, la cual puede ser xml o json.

	\item \textbf{props}: la propiedad del articulo al cual se desea consultar, en este caso se usa el valor \texttt{revisions} para poder extraer el historial de modificaciones del mismo.

	\item \textbf{rvprop}: atributos de la propiedad extraída, los incluidos para esta extracción son los siguientes: ids, flags, timestamp, user, userid, size, sha1, contentmodel, comment, parsedcomment, content, tags.

	\item \textbf{rvlimit}: permite la paginación de los resultados e indica la cantidad de resultados que se quieren obtener por página.

	\item \textbf{newer}: permite ordenar las revisiones de forma ascendete o descendiente acorde a la fecha de creación. Se ordenan de forma descendente usando el valor \texttt{newer}, de esta manera es más rápido extraer las revisiones mas recientes.
\end{itemize}

Justo antes de enviar la solicitud al API de wikipedia, se verifica que el artículo a consultar ya este almacenado en la base de datos, en caso de no estarlo, se almacena indicando la fecha de extracción.
Posteriormente, se extraen sus revisiones, y puesto que la respuesta del API coloca las revisiones mas recientes al principio, el extractor culmina su ejecución una vez determina que alcanzo una revisión que ya ha sido almacenada.
Al almacenar las revisiones en la base de datos, se actualiza el documento del artículo con la fecha de la ultima extracción y el identificador de la revisión extraída, y por ultimo, se actualiza el progreso de la tarea.

En la siguiente figura se puede apreciar un ejemplo de la consulta del progreso de las tareas:

\begin{figure}[H]
	\centering
		\includegraphics[width=0.8\textwidth]{figures/response_status}
	\caption{Respuesta a una petición para consultar de progreso de una tarea de extracción}
	\label{fig:response_status}
\end{figure}

La siguiente tabla muestra la historia asociada a la extracción de revisiones:

\input{tables/extract}

\input{chapters/revisit}


\subsubsection{Consultas}

El API consta de una ruta de acceso para consultar las revisiones extraídas y
los artículos asociados a través de las rutas \texttt{/api/v1/articles} y \texttt{/api/v1/revisions}.

Para consultar los artículos es posible hacer uso de diversos parámetros de búsqueda,
entre ellas están: los parámetros de paginación \texttt{page} y \texttt{page\_size},
y los parámetros de búsqueda que corresponden a los atributos del artículo,
tales como: \texttt{title}, \texttt{first\_extraction\_date}, \texttt{last\_extraction\_date}, \texttt{last\_revision\_extracted} y \texttt{locale}.

Un ejemplo de la respuesta obtenida por esta ruta puede ser apreciado en la siguiente figura:

\begin{figure}[H]
	\centering
		\includegraphics[width=0.8\textwidth]{figures/response_articles}
	\caption{Respuesta a una petición de consulta de artículos}
	\label{fig:response_articles}
\end{figure}

En el caso de la consulta de las revisiones, adicionalmente a los parámetros de búsqueda, esta el parámetro \texttt{sort} que permite, por fecha de extracción, el
ordenamiento ascendente o descendiente de las revisiones, y la elección de cuales
atributos se desean mostrar por cada revisión.

En la siguiente figura se puede apreciar una respuesta del servidor en la consulta de
revisiones:

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/response_revisions}
	\caption{Respuesta a una petición de consulta de revisiones}
	\label{fig:response_revisions}
\end{figure}

Por otro lado, existen mas rutas de acceso al API para la consulta de diversas métricas predefinidas, tales como: promedio, conteo de revisiones y moda.

El cálculo de estas métricas toma como parámetros múltiples atributos
como el título del artículo o la fecha de extracción, para poder filtrar el numero total revisiones de la consulta.
Estos parámetros pasan por un proceso de aceptación, en donde solo son aceptados aquellos que estén ya incluidos en una lista llamada lista blanca, aquellos parámetros que no estén dentro de esta lista son ignorados.
Después de esta etapa, se genera una tarea de Celery para poder ser procesada en segundo plano hasta obtener el resultado.

En las siguientes tablas se puede apreciar con detalle, por medio de sus historias, los filtros aceptados por cada métrica:

\input{tables/count}

Por último, el API provee la ruta \texttt{/api/v1/query} que permite ejecutar consultas a la base de datos directamente de los parámetros recibidos por la petición.
El cuerpo de las peticiones realizadas sobre esta ruta debe tener un formato json, en conjunto con el atributo de cabecera
\texttt{content-type} que debe tener como valor \texttt{application/json}.
La estructura del cuerpo de la petición debe coincidir con el formato aceptado para la creación de consultas por medio de PyMongo.
Un ejemplo de este formato puede ser apreciado en la siguiente figura, en donde cada clave de la dupla clave-valor del json representa una función de agregación valida de MongoDB:

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/query_body_format}
	\caption{Estructura del cuerpo de la petición a la ruta /api/v1/query}
	\label{fig:query_body_format}
\end{figure}

Adicionalmente, la ruta permite especificar la colección de la base de datos a la cual consultar, y de forma opcional,
el formato de fecha de alguna columna en caso de ser necesario.
Un ejemplo, del uso de estos parámetros es el siguiente, en donde se realiza una consulta de la colección llamada revisions:

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/query_url}
	\caption{Estructura del cuerpo de la petición a la ruta /api/v1/query}
	\label{fig:query_url}
\end{figure}

Por medio de esta ruta es posible realizar cualquier tipo de consulta sobre la base de datos, y así, obtener métricas
adicionales para su análisis posterior.

\input{tables/count}

\subsection{Almacenamiento}

Como fue mencionado en el Capitulo 2, la base de datos utilizada para almacenar las revisiones y artículos wiki, es MongoDB.
A través del proceso de extracción se almacenan dos colecciones:
revisions, que representa las revisiones o historial de modificaciones del artículo wiki;
y articles, que se refiere a los artículos de dichas revisiones.
La arquitectura elejida para esta solución contiene la cantidad de elementos mínimos para implementar un cluster de base de datos en MongoDB.
La misma cuenta con dos sets de replicas de shards, un set de replicas para el servidor de configuración, y por último,
un servidor de consultar.

En detalle, las colecciones están distribuidas entre dos fragmentos o shards que contienen un subgrupo de los datos fragmentados en el cluster, y la unión de estos subgrupos conforman la totalidad de los datos almacenados por la aplicación.
Esta división de datos, en grupos, puede ser apreciada en la siguiente figura:

\begin{figure}[H]
	\centering
		\includegraphics[width=0.4\textwidth]{figures/sharded_database}
	\caption{División de la base de datos en subgrupos}
	\label{fig:sharded_database}
\end{figure}

Cada uno de estos subgrupos, tal como lo demuestra la figura \ref{fig:diagram_api_1}, esta siendo almacenado un servidor físico diferente, garantizando el escalamiento horizontal.
Para la creación de estos grupos, el sharding se lleva a cabo a través del Hashed Sharding, de esta manera es
posible alcanzar una distribución homogénea de los datos entre los servidores.

\begin{figure}[H]
	\centering
		\includegraphics[width=0.5\textwidth]{figures/shards_distributed}
	\caption{Shards de la base de datos distribuidos físicamente}
	\label{fig:shards_distributed}
\end{figure}

Para poder crear un shard es necesario que este pertenezca a un set de replica de un mínimo de tres miembros,
por lo tanto, por cada shard se tienen tres servidores físicos que permiten la replicación de ese subgrupo de datos, garantizando
así la alta disponibilidad de los mismos.
Para poder fragmentar la base de datos, se necesita de un mínimo de dos shards, por lo que se tiene un total mínimo de 6 servidores
que solo cumplen las labores de fragmentación y replicación.

Para poder coordinar las funciones de estos shards, como la autenticación, almacenamiento de los meta datos y consultas sobre el cluster de shards,
es necesario hacer uso del servicio de un servidor de configuración.
Este servidor, al igual que los shards, debe pertenecer a un grupo de replicas, sumando tres servidores físicos mas, que ademas,
tienen la capacidad de recuperarse en caso de fallos, y nuevamente, permitir la alta disponibilidad.

Finalmente, se tiene un último servidor de base de datos cuya única labor es la ejecución de operaciones de lectura y escritura sobre la base de datos.
Este servidor es denominado servidor de consultas y no es necesaria su replicación.
Las aplicaciones clientes que deseen realizar alguna consulta sobre la base de datos, realizan sus peticiones únicamente a este servidor.
Así mismo, una vez es recibida la petición de consulta, este se comunica con servidor de configuración para que este le brinde apoyo a la hora de ejecutar las operaciones.
Con este último servidor, el conteo total de servidores de MongoDB para esta arquitectura es de diez nodos.

La siguiente figura muestra el diagrama de la arquitectura del cluster:

\begin{figure}[H]
	\centering
		\includegraphics[width=0.9\textwidth]{figures/mongo_cluster}
	\caption{Arquitectura del cluster de base de datos}
	\label{fig:mongo_cluster}
\end{figure}

\subsection{Docker}

Para poder poner en funcionamiento esta aplicación distribuida y el cluster de la base de datos, se hace uso de la herramienta Docker.
Docker permite levantar servicios que emulan un servidor físico, los cuales pueden ser conectados en una red para que puedan comunicarse y llevar a cabo todas las tareas de extracción y consultas.

Docker permite generar ambientes tanto de producción como de desarrollo, en este caso, se hace uso de la versión \texttt{17.12.0-ce} en conjunto con Docker-Compose para facilitar el levantamiento de los servicios en ambos ambientes.

En primera instancia se genera una imagen para la aplicación flask, los trabajadores de Celery y el resto de los servicios.
El nombre de las imágenes esta definido bajo el siguiente formato: \texttt{nombre:version}, y pueden ser generadas por medio de un archivo llamado Dockerfile, cuyo contenido puede ser apreciado a continuación:

\begin{lstlisting}[language=docker,caption={Contenido del archivo Dockerfile para la imagen del API},breaklines=true,label={code:dockerfile}]
# Dockerfile
FROM python:2.7-alpine

RUN mkdir /app
WORKDIR /app

COPY requirements.txt requirements.txt
RUN pip install -r requirements.txt

COPY . .

CMD python manage.py runserver --host 0.0.0.0 --port=80
\end{lstlisting}

Para los servicios de Nginx, RabbitMQ y MongoDB se hace uso de imágenes provenientes del repositorio oficial llamado Docker Hub,
las imágenes utilizadas son las siguientes: \texttt{rabbitmq:3.6.11}, \texttt{mongo:3.6.2} y \texttt{nginx:1.13.3}.

La configuración de los contenedores se realiza a través del archivo \texttt{docker-compose.yml}, en el cual se especifican
las imágenes a utilizar por cada servicio, variables de entorno, nombre de la red, entre otros.

Las imágenes de RabbitMQ y MongoDB tienen la característica que permite utilizar variables de entorno para establecer el usuario y clave de acceso sin ningún tipo de configuración manual por parte del desarrollador,
por lo tanto, las credenciales usadas para ambos servicios son agregadas en el archivo \texttt{docker-compose.yml} para restringir su acceso.
Por ejemplo, para RabbitMQ las credenciales son las siguientes: \verb|RABBITMQ_DEFAULT_USER=wiki| y \verb|RABBITMQ_DEFAULT_PASS=wiki123|.
Adicionalmente, se establecieron una serie reglas adicionales, como por ejemplo,
declaración de volúmenes para mantener la persistencia de datos y
la exposición de los puertos de RabbitMQ, MongoDB y Nginx;
lo cual permite a las aplicaciones externas acceder a estos servicios, y en el caso de Nginx, permite que otras aplicaciones puedan ejecutar peticiones al API.

Pare de la configuración de los servicios y contendedores de Docker puede ser apreciada a continuación:

\begin{lstlisting}[language=docker-compose-2,caption={Declaración de servicios con Docker Compose},breaklines=true,label={code:dockercompose}]
version: '3'

services:

    rabbit:
        image: rabbitmq:3.6.11
        restart: always
        hostname: rabbit
        environment:
            - RABBITMQ_DEFAULT_USER=wiki
            - RABBITMQ_DEFAULT_PASS=wiki123
        ports:
            - "5673:5672"
        networks:
            - wiki_network
        volumes:
            - 'wiki_rabbit:/data'
\end{lstlisting}
