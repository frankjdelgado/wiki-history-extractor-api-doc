\section{Problema}

\subsection{Planteamiento Del Problema}

Un \emph{wiki} es una página web que permite la colaboración de múltiples usuarios para 
la creación y modificación de contenido que hacen referencia a un tema en particular y
que pueden estar relacionados entre ellos a través de hipervínculos.

Un wiki es ejecutado por el medio del uso software wiki que permite a los usuarios la edición
de estás paginas. El contenido de las páginas, también llamados artículos, es almacenado en conjunto
con su historial de cambios en una base de datos. De esta forma se facilita recuperar el estado
anterior de cualquier artículo y visualizar diversos datos de una actualización tal como el
nombre del autor o la fecha de edición.

Hoy en día el proyecto enciclopedia en línea Wikipedia es una de las paginas web basadas en wiki
mas populares, albergando miles de colecciones de artículos wiki en múltiples idiomas \cite{3}.

La aplicación Wiki-Metrics-UCV tiene como objetivo la extracción de estos historiales de
de los artículos wiki para su almacenamiento y posteriormente usar sus datos para 
el cálculo de diversas métricas. Wiki-Metrics-UCV hace uso de técnicas de extracción web (web
scraping) sobre los artículos wiki y, a través de un análisis sintáctico, almacena los datos
obtenidos en una base de datos no relacional \cite{4}.

El proceso de extracción de historiales de artículos wiki puede llegar a tomar
una cantidad considerable de tiempo, no sólo por la cantidad entradas,
sino por el número de documentos que un repositorio wiki alberga. 

En aplicaciones como Wiki-Metrics-UCV, que corresponden a una solución de cómputo y almacenamiento
centralizado, existen diversas limitaciones que se presentan en la solución. Entre ellas es
que el proceso se realiza de manera secuencial, lo cual obliga a la aplicación a terminar
de procesar un artículo antes de pasar al siguiente, y como consecuencia, el tiempo
empleado para procesar grandes volúmenes de historiales no se encuentra dentro de un
intervalo aceptable para la cantidad de revisiones previstas a procesar.

Adicionalmente, los repositorios wiki tales como Wikipedia limitan la cantidad de peticiones HTTP que se 
realizan sobre sus servicios. Por lo tanto, existe la posibilidad de que
un sistema distribuido lleve a cabo una cantidad peticiones concurrentes que puedan
ser vistas como un ataques de denegación de servicio (DDoS).

Dado que en la solución implementada por Wiki-Metrics-UCV se establecen tiempos definidos
de espera entre petición pensados únicamente para el caso centralizado, es necesario
considerar y adaptar ciertos aspectos para su adaptación en un escalamiento horizontal,
tales como: la cantidad de nodos que ejecutan peticiones HTTP sobre un URL; la
concurrencia y el tiempo de espera entre cada petición; y por último los permisos
presentes en los términos y condiciones de uso de cada repositorio de artículos wiki en
el aspecto de la extracción web.

En términos legales, y en las clausulas de los términos y condiciones de uso, cada repositorio
wiki indican como el uso de sus servicios y la extracción de sus datos públicos puede ser considerado
inapropiado. En el caso de Wikipedia, por medio de sus términos de uso se indica que un individuo debe 
evadir cualquier actividad, ya sea legal o ilegal, que pueda comprometer su infraestructura tecnológica o 
violar derechos de autor \cite{7}. 

Dado que Wiki-Metrics-UCV no recolecta la información contenida en un wiki, ni
existe una cláusula específica sobre la recolección de datos de los historiales de edición,
las actividades ejecutadas por esta aplicación no violentan los términos de uso de
Wikipedia mientras que las peticiones HTTP realizadas no comprometan su sistema.
Actualmente Wikimedia, quien es el creador del proyecto Wikipedia, ofrece una
herramienta para la recolección y consulta de datos de un wiki, incluyendo su historial,
llamado MediaWiki. MediaWiki provee de una interfaz de programación de aplicaciones
(Aplication Program Interface - API) que puede ser usada por la aplicación de un tercero
para consultar estos datos. Este API se presta como alternativa a las técnicas de
web scraping usadas por Wiki-Metrics-UCV y tiene ciertas limitaciones con respecto al límite
de peticiones HTTP ejecutadas sobre este servicio, similares a las relacionadas con la
extracción web. A pesar de que MediaWiki no define límites específicos sobre estas
peticiones, sí indica que la cantidad y frecuencia de las mismas sean moderadas,
especialmente si las peticiones son ejecutadas de forma paralela. En caso de que estas
consultas comprometan el sistema, MediaWiki especifica que sus administradores de
sistema (sysadmin) están en la potestad de bloquear la dirección IP de quien realice
estas peticiones \cite{8}.

La adaptación de Wiki-Metrics-UCV a un sistema distribuido disminuye varias de las limitaciones previamente
mencionadas. Con la presencia de varios nodos en el sistema es posible procesar diversos 
artículos de forma paralela, evitando cuellos de botella, y ademas, dividir la carga de almacenamiento 
entre los componentes del sistema.

Este escalamiento horizontal, tal como lo explica el teorema CAP, trae consigo problemas 
adicionales e  intrínsecos de todo sistema distribuido. Es imposible para un servicio garantizar las tres
siguientes características: consistencia, disponibilidad y tolerancia a fallos. A pesar de que estas 
propiedades son esenciales, es necesario identificar qué características deben ser acopladas a
esta nueva solución y cual debe ser sacrificada \cite{6}.

Por otra parte, teniendo en cuenta que con un escalamiento horizontal existen N
nodos independientes, es innecesario mantener una comunicación entre ellos para
coordinar sus acciones y cumplir con la meta establecida. Así mismo, es indispensable
definir las políticas y algoritmos para la elección de coordinadores del sistema
distribuido, el cual tiene como objetivo la asignación de los artículos wiki que procesa
cada uno de los nodos para evitar consultas redundantes sobre un mismo URL, y la
planificación de las actualizaciones en la base de datos. Adicionalmente, es importante
considerar el manejo de recursos críticos dentro de la comunicación, para lograr una
adecuada sincronización entre los nodos del sistema, y así evitar interbloqueos en la
comunicación entre ellos. Otro aspecto a tomar en cuenta, es que a medida que la
cantidad de nodos crezca (en caso del tamaño del escalamiento), la comunicación
existente en el sistema también se incrementará; por ello, es necesario considerar el
sistema de comunicación a emplear, y estimar su fiabilidad en casos adecuados a lo que
se determinará en relación al número de nodos para la solución.

Por último, se debe tomar en cuenta la arquitectura y la forma en la cual los datos
serán distribuidos entre los componentes del sistema. Determinar bajo que estrategia
los datos serán fragmentados y si será posible ofrecer tolerancia a fallos, consistencia y/o alta
disponibilidad. 

\subsection{Justificación}

Actualmente la extracción de datos es una práctica difuminada por todo el mundo, la
cual, bajo algunas circunstancias, roza con los aspectos legales por derechos de autor y
políticas de privacidad. Sin embargo, el uso de esta técnica, bajo lineamientos legales y con
moderación, permite una gran ventaja para la recolección de datos que puede ser
utilizados en un sinfín de aplicaciones.

En el caso de los artículos wiki de la Fundación Wikimedia, existe un campo poco
explorado con respecto a las estadísticas de los artículos, las cuales permitirían resaltar
características difíciles de visualizar a primera vista. De estos artículos es posible
obtener información muy relevante a través del estudio de su historial de revisiones, por ejemplo:
el colaborador que más ha editado un artículo en particular, las fechas y
horas cuando más se ha editado un artículo o a qué país o continente pertenecen las
personas que más han colaborado en los artículos. Incluso, es posible determinar ciertos
patrones de interés de los usuarios sobre un tema, lo cual puede ser usado por diversos
analistas para elaborar alguna estrategia de mercadeo y publicidad. La cantidad de
revisiones realizadas sobre un artículo wiki reflejan el uso y la manipulación de datos
sobre un recurso lo cual puede ser usado para determinar tendencias sobre el tema del
artículo.

Debido a las limitaciones de un sistema centralizado, surge la necesidad implementar una nueva solución
que permita ampliar las capacidades de almacenamiento y disminuir los cuellos de botella que se puedan
producir durante la extracción de los historiales. Por medio de de un escalamiento horizontal
es posible hacer uso de un grupo de computadores que comparten la carga de todas las tareas a ejecutar,
permitiendo de esta manera incrementar la cantidad de datos que se pueden almacenar y procesar.

\subsection{Objetivos}

\subsubsection{Objetivo General}

Desarrollar una aplicación distribuida para la extracción, almacenamiento y
procesamiento del historial de los artículos wiki basados en Wikimedia a través de
técnicas de web scraping y el uso de Wikimedia API.

\subsubsection{Objetivo Específicos}

Desarrollar un algoritmo de web scraping de bajo impacto sobre las revisiones
de un artículo wiki.

\begin{itemize}
\item Desarrollar de un módulo para la recolección y de consulta de los datos sobre el
servicio API de Wikimedia.

\item Diseñar un modelo de datos para el almacenamiento de los historiales de los
artículos wiki.

\item Implementar y configurar la topología de sharding de MongoDB para el sistema
distribuido.

\item Configurar los nodos de MongoDB dentro del sistema distribuido para la
implementación de réplicas.

\item Adaptar los algoritmos de revisita sobre los artículos wiki basados en la solución
brindada por Wiki-Metrics-UCV.

\item Diseñar y configurar los métodos de asignación de tareas para el balance de
carga entre los nodos del sistema.

\item Desarrollar los algoritmos de procesamiento de los datos almacenados para la
visualización de estadísticas de los artículos wiki.

\item Implementar un conjunto de pruebas sobre los módulos de extracción de datos,
almacenamiento, replicación y procesamiento de los mismos.

\item Desarrollar un API que permita a aplicaciones de terceros consultar los
historiales de los artículos wiki.

\end{itemize}